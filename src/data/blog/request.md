---
author: 'Lanxinmob'
title: '暑期日记6'
postSlug: 'summer-diary-6'
featured: false
draft: false
tags:
  - '笔记'
  - '计算机网络'
ogImage: ''
description: '延迟'
pubDatetime: 2025-07-11T09:00:00Z
---

## 系统延迟

- 从纳秒到秒建立一种对延迟时间量级的直觉
#### `1ns`
- 访问 cpu 寄存器（Register）的时间是亚纳秒。
#### `1-10ns`
- 访问 `L1 and L2 cache` ，或昂贵的cpu操作如分支预测错误的惩罚是二十个cpu时钟周期是`1-10ns`级。
#### `10-100ns`
- 访问 `L3 cache(shared)`，苹果M1的现代处理器访问主内存时间位于这个范围的慢端。
### 小结

- CPU 除了直接使用寄存器，会优先在离自己最近的 `L1` 缓存中寻找数据，找不到再去` L2`，再到` L3`，最后才去访问主内存。
- 主内存就是**RAM**，我们常说的内存条，他是**易失性**的，一旦断电所有数据都会消失。相比硬盘或SSD读取速度更快，但比CPU缓存慢得多。
- 离 CPU 越近的存储越快，但容量越小、成本也越高。
- 每个核有自己的 `L1L2` 但多个核共用一个`L3`可以提高多核协作的效率。

#### `100-1000ns` 
- 在Linux系统，一个简单的系统调用要几百纳秒，这仅是进入内核并返回的直接成本，不包括执行系统调用的成本
  - 对一个64位数字进行MD5哈希大约需要200纳秒
#### `1-10us`
- 在linux内核间进行上下文切换至少需要几微秒，如果新线程需要将数据从内存调入，如将64千字节数据从一个主内存位置复制到另一个主内存位置需要几微秒。
#### `10-100us`

- 像 `Nginx` 这样的网络代理处理一个典型的HTTP请求大概15微秒，从主内存顺序读取一百万字节的数据大约需要15微秒，SSD的读取延迟在这个范围内，可以读一个8K页大概`100us`。

#### `100-1000us`

- SSD的写入延迟是读取延迟的十倍，位于这个范围的上限，写入一个页面大概需要`1ms`。现代云提供商的区内网络往返时间需要几百微秒，现在往往接近快速端，甚至有些记录在`100us`内。
### 小结
- **页面(page)**是现代操作系统进行内存管理的基本单元，是一个固定大小的虚拟内存块，简化了内存地址的映射和管理，提高了效率。
- SSD被组织成页和块，一个块由很多页组成，读取时控制器可以直接读取任意一页内容，写入时需要先读取整个块到内存，修改某一页，找到一个空白块写入，更新地址映射表指向新的位置，旧的块被标记为失效等待后续垃圾回收机制处理。
- 区域是一个数据中心，有多个可用区，每个可用区有许多服务器，区内往返指**Intra-zone RTT (区内往返)**，跨区往返延迟为毫秒级。
#### `1-10ms`

- 典型的 `memcahe` 或 `Redis GATT` 操作被客户端测得大约需要`1ms`。硬盘寻道时间大约`5ms`。

#### `10-100ms`

- 美国东西海岸或美国东海岸到欧洲间的网络往返时间，从主内存顺序读`1GB` 数据。

#### `100-1000ms`

- 用慢速哈希函数`bcrypt` 加密一个密码需要 ` 300ms`。TLS 握手通常在 `250-500ms`。从固态硬盘顺序读 `1GB` 数据。美国西海岸到新加坡的网络往返时间。

#### `1s`

- 在同一云区域内传输 `1GB` 数据大约需要10s。

---

参考：[Latency Numbers Programmer Should Know: Crash Course System Design #1](https://www.youtube.com/watch?v=FqR5vESuKe0&t=1s)