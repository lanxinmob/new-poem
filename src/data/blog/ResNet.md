---
author: 'Lanxinmob'
title: 'ResNet'
postSlug: 'summer-diary-17'
tags:
  - '笔记'
ogImage: ''
description: '李沐论文精读 ResNet'
pubDatetime: 2025-07-24T09:00:00Z
toc: true
---

## [ResNet]()计算机视觉半边天

[Deep Residual Learning for Image Recognition - Kaiming He, X. Zhang, Shaoqing Ren, Jian Sun](https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d) (Citations: 195310) [PDF](D:\poem\src\data\blog\download\1512.03385.pdf)

- 那些经典的文章不一定要原创地提出什么东西或技术，很可能就是将以前的一些东西巧妙组合起来，解决一个现在比较难的问题，受到大家认可就能成为一个经典的工作。

- bottleneck的设计极大地减少了参数数量和计算复杂度（FLOPs）。


>
> 如果没有这个设计的情况下两个输入输出都为256通道的卷积层时
>
> 第一个卷积层参数：`(3×3 核大小) × 256 (输入通道) × 256 (输出通道) = 589,824`
>
> 第二个卷积层参数：`(3×3 核大小) × 256 (输入通道) × 256 (输出通道) = 589,824`
>
> 总参数量 ≈ 1,180,000

> 有瓶颈设计下，输入输出通道仍为256但是先用一个 **`1x1` 的卷积**，将通道数从 256 减少到 64。
>
> 然后用一个 `3x3` 的卷积来处理这个特征图，输入和输出通道都是 64。
>
> 最后再用一个 **`1x1` 的卷积**，将通道数从 64 恢复到原来的 256。
>
> 降维层 (`1x1`) 参数：`(1×1 核大小) × 256 (输入通道) × 64 (输出通道) = 16,384`
>
> 核心卷积层 (`3x3`) 参数：`(3×3 核大小) × 64 (输入通道) × 64 (输出通道) = 36,864`
>
> 升维层 (`1x1`) 参数：`(1×1 核大小) × 64 (输入通道) × 256 (输出通道) = 16,384`
>
> 总参数量 = 16,384 + 36,864 + 16,384 = 69,632

- **Shortcut（快捷连接）**：它的主要作用是解决“梯度消失”问题。
- 这里的梯度相比没有残差时只是一个连乘，还加上了一个本身的梯度这样就能保持梯度一直够大，就能一直训练下去。

- 只要保证梯度一直够大，其实你最后的结果就会比较好，那些其实不算收敛只是梯度没了跑不动了。

- 之前认为只要保持前面那些层的优势再加上一些层就算是恒等映射也不会变差，但是事实上没有引导它就是走不到那条路。ResNet加上这些东西其实是降低了模型的复杂度，比没加这些东西更容易走到那个想要的道路，即寻找最优解是更容易的。

- 回到重新思考过拟合、泛化的问题，为什么像现在transformer 上千亿参数没有过拟合，在海量数据面前表现强大的泛化能力还是一个开放的问题。

- 这里残差是作用在feature维度，让网络更容易训练，**gradiant boosting**的residual是在标签上做residual，不断预测之前所有模型加起来的预测误差，让模型迭代式地修正误差。

